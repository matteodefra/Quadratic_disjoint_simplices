@article{JMLR:v12:duchi11a,
    author  = {John Duchi and Elad Hazan and Yoram Singer},
    title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
    journal = {Journal of Machine Learning Research},
    year    = {2011},
    volume  = {12},
    number  = {61},
    pages   = {2121-2159},
    url     = {http://jmlr.org/papers/v12/duchi11a.html}
}


@Article{Frangioni2017,
    author  = {Frangioni, Antonio
               and Gendron, Bernard
               and Gorgone, Enrico},
    title   = {On the computational efficiency of subgradient methods: a case study with Lagrangian bounds},
    journal = {Mathematical Programming Computation},
    year    = {2017},
    month   = {Dec},
    day     = {01},
    volume  = {9},
    number  = {4},
    pages   = {573-604},
    abstract= {Subgradient methods (SM) have long been the preferred way to solve the large-scale Nondifferentiable Optimization problems arising from the solution of Lagrangian Duals (LD) of Integer Programs (IP). Although other methods can have better convergence rate in practice, SM have certain advantages that may make them competitive under the right conditions. Furthermore, SM have significantly progressed in recent years, and new versions have been proposed with better theoretical and practical performances in some applications. We computationally evaluate a large class of SM in order to assess if these improvements carry over to the IP setting. For this we build a unified scheme that covers many of the SM proposed in the literature, comprised some often overlooked features like projection and dynamic generation of variables. We fine-tune the many algorithmic parameters of the resulting large class of SM, and we test them on two different LDs of the Fixed-Charge Multicommodity Capacitated Network Design problem, in order to assess the impact of the characteristics of the problem on the optimal algorithmic choices. Our results show that, if extensive tuning is performed, SM can be competitive with more sophisticated approaches when the tolerance required for solution is not too tight, which is the case when solving LDs of IPs.},
    issn    = {1867-2957},
    doi     = {10.1007/s12532-017-0120-7},
    url     = {https://doi.org/10.1007/s12532-017-0120-7}
}

@article{dAntonio2009ConvergenceAO,
    title   = {Convergence Analysis of Deflected Conditional Approximate Subgradient Methods},
    author  = {G. d'Antonio and A. Frangioni},
    journal = {SIAM J. Optim.},
    year    = {2009},
    volume  = {20},
    pages   = {357-386}
}

@inproceedings{NIPS2009_7cce53cf,
    author = {Xiao, Lin},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Dual Averaging Method for Regularized Stochastic Learning and Online Optimization},
    url = {https://proceedings.neurips.cc/paper/2009/file/7cce53cf90577442771720a370c3c723-Paper.pdf},
    volume = {22},
    year = {2009}
}

@inproceedings{inproceedings,
    booktitle = {Composite Objective Mirror Descent},
    author = {Duchi, John and Shalev-Shwartz, Shai and Singer, Yoram and Tewari, Ambuj},
    year = {2010},
    month = {12},
    pages = {14-26},
    title = {Composite Objective Mirror Descent}
}

@Article{Nesterov2009,
    author={Nesterov, Yurii},
    title={Primal-dual subgradient methods for convex problems},
    journal={Mathematical Programming},
    year={2009},
    month={Aug},
    day={01},
    volume={120},
    number={1},
    pages={221-259},
    abstract={In this paper we present a new approach for constructing subgradient schemes for different types of nonsmooth problems with convex structure. Our methods are primal-dual since they are always able to generate a feasible approximation to the optimum of an appropriately formulated dual problem. Besides other advantages, this useful feature provides the methods with a reliable stopping criterion. The proposed schemes differ from the classical approaches (divergent series methods, mirror descent methods) by presence of two control sequences. The first sequence is responsible for aggregating the support functions in the dual space, and the second one establishes a dynamically updated scale between the primal and dual spaces. This additional flexibility allows to guarantee a boundedness of the sequence of primal test points even in the case of unbounded feasible set (however, we always assume the uniform boundedness of subgradients). We present the variants of subgradient schemes for nonsmooth convex minimization, minimax problems, saddle point problems, variational inequalities, and stochastic optimization. In all situations our methods are proved to be optimal from the view point of worst-case black-box lower complexity bounds.},
    issn={1436-4646},
    doi={10.1007/s10107-007-0149-x},
    url={https://doi.org/10.1007/s10107-007-0149-x}
}

@misc{chen2011projection,
    title={Projection Onto A Simplex}, 
    author={Yunmei Chen and Xiaojing Ye},
    year={2011},
    eprint={1101.6081},
    archivePrefix={arXiv},
    primaryClass={math.OC}
}

@article{OPT-003,
    url = {http://dx.doi.org/10.1561/2400000003},
    year = {2014},
    volume = {1},
    journal = {Foundations and Trends® in Optimization},
    title = {Proximal Algorithms},
    doi = {10.1561/2400000003},
    issn = {2167-3888},
    number = {3},
    pages = {127-239},
    author = {Neal Parikh and Stephen Boyd}
}

@article{GUBIN19671,
    title = {The method of projections for finding the common point of convex sets},
    journal = {USSR Computational Mathematics and Mathematical Physics},
    volume = {7},
    number = {6},
    pages = {1-24},
    year = {1967},
    issn = {0041-5553},
    doi = {https://doi.org/10.1016/0041-5553(67)90113-9},
    url = {https://www.sciencedirect.com/science/article/pii/0041555367901139},
    author = {L.G. Gubin and B.T. Polyak and E.V. Raik},
    abstract = {MANY mathematical and applied problems can be reduced to finding some common point of a system (finite or infinite) of convex sets. Usually each of the sets is such that it is not difficult to find the projection of any point on to this set. In this paper we shall consider various methods of finding points from the intersection of sets, using projection on to a separate set as an elementary operation. The strong convergence of the sequences obtained in this way is proved. Applications are given to various problems, including the problem of best approximation and problems of optimal control. Particular attention is paid in the latter case to problems with restrictions on the phase coordinates.}
}

@misc{nonnegative-orthant,
    author = {Andersen, Ang},
    title = {Projection onto nonnegative orthant, rectangular box and polyhedron},
    year = {2020},
    note = {First draft: March 19, 2020; Last update: December 23, 2020. Université de Mons, \url{https://angms.science/doc/CVX/Proj_nonnegBoxpoly.pdf}}
}

@misc{symplex,
    author = {Andersen, Ang},
    title = {Projection onto symplex},
    year = {2020},
    note = {First draft: June 15, 2020; Last update: December 24, 2020. Université de Mons, \url{https://angms.science/doc/CVX/Proj_simplex.pdf}}
}

@article{convexjl,
    title = {Convex Optimization in {J}ulia},
    author = {Udell, Madeleine and Mohan, Karanveer and Zeng, David and Hong, Jenny and Diamond, Steven and Boyd, Stephen},
    year = {2014},
    journal = {SC14 Workshop on High Performance Technical Computing in Dynamic Languages},
    archivePrefix = "arXiv",
    eprint = {1410.4821},
    primaryClass = "math-oc",
}