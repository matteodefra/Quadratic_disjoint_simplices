n=1000, K=20, Rule 1
After multiple tries, I discovered that the optimal value of \alpha=0.8 and the following method could be applied:
We start with the stepsize rule 2 (the standard DSS) and when reaching a relative gap of 1e4 we switch to the more stable Polyak stepsize. In this way, the convergence is a lot faster since we employ very big steps and then a sort of regularization.
In this case, the \alpha=0.8 or 0.9, the \beta=1, the primal is solved using YALMIP and the value is reported in Julia.
The tolerance in this case is set to \tau=1e-8, where value start oscillating.

n=5000, K=10, Rule 1
In this case, we discovered that an optimal \alpha=0.3, since the same as before happens. We reach a 1e4 gap and then we switch to the optimal step rule of Polyak.
In this case the tolerance \tau=2e-6 since being magnitude of the problem bigger the gap start oscillating before


n=5000, K=10, Rule 2
Rule 2 seems to be not capable of learning anything.
Try to change the value of \delta -> using standard \delta=1, one of the possible rules changing the hyperparameters does not decrease


n=5000, K=10, Rule 3
In rule 3 I notice how changing the value of \delta impact the rate of updates in the convergence. Using a > 1 \delta suddenly cause big steps. Instead with a low value of it we have a more smooth optimization.
Also in this case if we use an high \delta we make big steps, the algorithm seems prone to diverge into the negative gap. To mitigate this phenomena, the subgrad direction is changed when this happens
