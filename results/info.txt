n=1000, K=20, Rule 1
After multiple tries, I discovered that the optimal value of \alpha=0.8 and the following method could be applied:
We start with the stepsize rule 2 (the standard DSS) and when reaching a relative gap of 1e4 we switch to the more stable Polyak stepsize. In this way, the convergence is a lot faster since we employ very big steps and then a sort of regularization.
In this case, the \alpha=0.8 or 0.9, the \beta=1, the primal is solved using YALMIP and the value is reported in Julia.
The tolerance in this case is set to \tau=1e-7, where value start oscillating.

n=5000, K=10, Rule 1
In this case, we discovered that an optimal \alpha=0.3, since the same as before happens. We reach a 1e4 gap and then we switch to the optimal step rule of Polyak.
In this case the tolerance \tau=2e-6 since being magnitude of the problem bigger the gap start oscillating before

n=5000, K=10, Rule 2
Rule 2 seems to be not capable of learning anything.
Try to change the value of \delta -> using standard \delta=1, one of the possible rules changing the hyperparameters does not decrease

n=5000, K=10, Rule 3
In rule 3 I notice how changing the value of \delta impact the rate of updates. The constant stepsize is needed otherwise the convergence is very very slow.
By adjusting the hyperparameters and experimenting, I reached that using a \delta = 1e-16 (almost zero) and changing $h$ during learning achieves the dual objective. In particular, h assume the following values:
Starting point, h=200 -> this allows for big steps (our function value is very high)
When the gap becames smaller than 1e3, we set h = 25, to smoothen the steps
Then gap < 5e2 -> h = 5, we are reaching the tail
gap < 1e2 -> h = 2, bring the steps to small magnitude 
gap < 1e1 -> h = 1e-2, bring the steps to small magnitude 
gap < 1e-1 -> h = 1e-3, bring the steps to small magnitude 